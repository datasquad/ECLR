---
title: "R Variable Selection, Lasso and Ridge Regression"
output: webexercises::webexercises_default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(webexercises)
```


```{r, echo = FALSE, results='asis'}
# Uncomment to change widget colours:
#style_widgets(incorrect = "goldenrod", correct = "purple")
```

# Introduction

Here we will demonstrate the use of LASSO and Ridge regressions to deal with situations in which you estimate a regression model that contains a large amount of potential explanatory variables

```{r}
# load packages
library(car)
#library(carData,curl)
library(AER)
library(glmnet)
library(hdm)
library(stats)
require(graphics)
library(leaps)
library(tidyverse)
library(haven)
library(stargazer)
library(ggplot2)
library(sandwich)
library(coefplot)
```

Let's load the dataset which contains an extract from the Panel Study of Income Dynamics (PSID). The datafile here is saved as a text file which can be imported using the `read.delim` function.

```{r}
## load data set 
PSID1 <- read.delim("../data/PSIDsmall.txt",header=TRUE)
```

Let's see what variables are contained in this dataset.

```{r}
names(PSID1)
```
```{r}
summary(PSID1)
```

Let's look at the distribution of wages (measured as hourly wages).

```{r}
phist <- ggplot(PSID1, aes(x=WAGE)) + geom_histogram()
phist
```

You can see that there are a few individuals with very large hourly wages. Let us exclude wages that are larger than USD200 per hour and have another look at the wage distribution.

```{r}
PSID <- PSID1 %>% filter(WAGE <200)
phist <- ggplot(PSID, aes(x=WAGE)) + geom_histogram()
phist

```

# Exploring the variables

These are observations from the PSID and to  do serious work with this dataset we need to understand the variables. The PSID surveys household on their income but also a number of household and personal characteristics. The following [variable search page](https://simba.isr.umich.edu/DC/s.aspx) can be useful here. The main person interviewed is the household head, if there is a couple this was typically the man. You will see some variables which exist in two versions, for instance `AGE` and `AGEW`. The former refers to the household head and the latter to the wife's age. You can tell that this labeling is the result of patriarchal thinking. In later versions of the PSID this labeling has changed to household head and (where appropriate) the spouse.

Variable Name | Definition
------------- | -------------
WAGE | Hourly wage of household head
AGE | Age of household (HH) head
AGE2 | Squared Age of HH head
AGEW | Age of wife (0 if no wife/spouse)
AGEY | Age of youngest child (0 if no child, children aged 0 and 1 will have an entry of "1")
CHILDREN | number of children
DEGREE | highest degree earned (0 = not applicable, 1 = Associate, 2 = Bachelor, 3 = Master, 4 = PhD, 5 = Law, LLB, 6 = Medical, MD ), several reasons for 0, e.g. no college attendance GRADCOLL = 5
DEGREEW | highest degree earned (0 = not applicable, 1 = Associate, 2 = Bachelor, 3 = Master, 4 = PhD, 5 = Law, LLB, 6 = Medical, MD ), several reasons for 0, e.g. no college attendance GRADCOLLW = 5 or no spouse
FEDUCA | Years of foreign education of father, 0 if no education abroad
FEDUCH | Father completed education in the US (1 = 0 to 5 grades, 2 = 6 to 8 grades, 3 = 9 to 11 grades, 4 = 12 grades, High School, 5 = HS plus further non-academic education, 6 = HS + some college but no completed degree, 7 = 15 to 16 years, College degree, 8 = 17+ years, higher than College degree, 0 not applicable)
MEDUCA | Years of foreign education of mother, 0 if no education abroad
MEDUCH | Mother completed education in the US (1 = 0 to 5 grades, 2 = 6 to 8 grades, 3 = 9 to 11 grades, 4 = 12 grades, High School, 5 = HS plus further non-academic education, 6 = HS + some college but no completed degree, 7 = 15 to 16 years, College degree, 8 = 17+ years, higher than College degree, 0 not applicable)
GRADCOLL | Graduated from college (1 = Yes, 5 = no, 0 = not applicable)
GRADCOLLW | Graduated from college (1 = Yes, 5 = no, 0 = not applicable or no wife/spouse)
GRADHS | HH head graduate from High School (1 = Graduated, 2 = General Educational Diploma, 3 = Neither, 4 = not applicable)
GRADHSW | Wife graduate from High School (1 = Graduated, 2 = General Educational Diploma, 3 = Neither, 4 = not applicable or no wife/spouse)
HAPPINESS | During the past 30 days, how much of the time did you feel extremely happy (1 = All of the time to 5 = None of the time)
MARRIED | 1 = Married, 2 = Never Married, 3 = Widowed, 4 = Divorced, 5 = Separated
SEX | 1 = Male, 2 = Female
YEARSCOLL | Highest year of college completed
STATE | [State identifier](https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_abbreviations), e.g. 5 = Arkansas

# Modeling wage variation

Let us start by running a simple regression of wages against the `GRADCOLL` variable

```{r}
OLS1<-lm(WAGE~GRADCOLL, data = PSID)
stargazer(OLS1, type = "text")
```

```{r}
ggplot(PSID, aes(x = GRADCOLL, y = WAGE)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

GRADCOLL does not appear to explain the variation in wages (it appears as statistically insignificant). But the plot above also reveals another aspect of the data. There are only three possible values (0,1 and 5) for the `GRADCOLL` variable. It is defined as as a numeric variable, but really is a categorical variable (see the above table). If we treat it as numeric, as we have done above, we assume that a response of 1 (did receive a college degree is one unit better or worse than a response of 0 (inapplicable question) and a response of 5 (no college degree) is five times as good or bad as a response of 1. That doesn't make sense as the question is really a categorical variable.

Let's define the variables as categorical variable

```{r}
PSID$GRADCOLL <- as.factor(PSID$GRADCOLL)
```

Now, let's re-run the regression with `GRADCOLL` as a categorical variable:


```{r}
OLS1b<-lm(WAGE~GRADCOLL, data = PSID)
stargazer(OLS1b, type = "text")
```

What this regression has done is to create a dummy variable for all possible outcome values of the `GRADCOLL` variable, other than for a base group, here `GRADCOLL=0`. Now we can see that there is a positive effect of having a college degree, but also that not having one has a positive effect relative to the reference group which are individuals where the question was not applicable. One would really have to understand what the composition of this reference group is. It may for instance include immigrants.

Let's look at other variables which we may have to convert to categorical variables.

Which of the following variables should really be defined as categorical?

`r mcq(c( "AGE", answer ="gradhs", "WAGE"))` 

So, let's turn the following variables into factors.

```{r}
cols <- c("DEGREE","DEGREEW", "FEDUCA", "FEDUCH", "GRADCOLLW","GRADHS","GRADHSW","MARRIED","MEDUCA","MEDUCH","SEX","STATE")
PSID <- PSID %>% mutate_at(cols, factor)
```

In fact, you also need to carefully think about how to code, say the age of the spouse `AGEW`. To understand this it is useful to look at which values this variable takes.

```{r}
table(PSID$AGEW)
```

As you can see there are 1355 responses where the age of the wife/spouse is coded as "0". This indicates that there are 1355 households where there is no wife or spouse. If we were to use the `AGEW` variable as a numeric variable (which it is coded as) then R would take the value of "0" literally. To avoid this we may want to create a new variable which indicates whether there is a spouse in the household (`SPOUSE`). That variable will be TRUE if `AGEW>17`.

```{r}
PSID <- PSID %>% mutate(SPOUSE = (AGEW>17))
```

A similar story applies to the `AGEY` variable which indicates the age of the youngest child. We can look at a table indicating how often we get certain combinations of the `AGEY` and `CHILDREN` variable.

```{r}
table(PSID$AGEY,PSID$CHILDREN, dnn = c("Age of youngest child","Number of children"))
```
You can see that `AGEY=0` corresponds to no child being in the household. We therefore define the variable `CHILD` as follows

```{r}
PSID <- PSID %>% mutate(CHILD = (CHILDREN>1))
```

Now we can include multiple variables that may help explain variation in wages, such as AGE, MARRIED etc. Therefore define the model with all controls.

```{r, results='hide'}
reg_wage_all_control <- WAGE~AGE+AGE2+AGEW+AGEY+CHILDREN+DEGREE+DEGREEW+
  FEDUCA+FEDUCH+GRADHS+GRADHSW+HAPPINESS+MARRIED+MEDUCA+
  MEDUCH+SEX+STATE+YEARSCOLL
# does not use GRADCOLL and GRADCOLLW as that info is colinear with DEGREE and DEGREEW
# check table(PSID$GRADCOLL,PSID$DEGREE) to see why
# run multiple linear regression
OLS2<-lm(reg_wage_all_control, data = PSID)
stargazer(OLS2, type = "text")
```

Why did the above regression exclude the `GRADCOLL` and `GRADCOLLW` variable?

`r mcq(c( answer = "As GRADCOLL1 = DEGREE1 + ... + DEGREE6", "As GRADCOLL1 = DEGREE1", "As GRADCOLL5 = DEGREE1 + ... + DEGREE6", "As GRADCOLL5 = DEGREE5"))` 

The regression output is not shown here as the regression contains more than 200 coefficients. Recall that dummy variables are created for all possible outcomes for each factor variable, so as there 52 possible outcomes for the variable `STATE` there are 52-1 variables included for the `STATE` variable alone.

Of course there are many of these variables that are not significant. For instance, let's test whether the coefficients relating to the `MEDUCH` variable are statistically significantly different from 0.

```{r}
test0 <- names(coef(OLS2)) # get all coefficient names
test0 <- test0[grepl("MEDUCH", test0)]
lht(OLS2, test0)
```
The p-value is larger than 0.8 and hence the null hypothesis that these coefficients are 0 cannot be rejected.

# Variable Selection

The mechanics of the forward, backward and best subset selection methods are described in more detail [here](https://datasquad.github.io/ECLR/R_VariableSelection_Credit/R_VariableSelection_Credit.html).

## Forward stepwise selection

```{r}
PSID_reg <- PSID %>% select(-c(GRADCOLL,GRADCOLLW))  # to exclude the colinear variables
regfit.forw = regsubsets(WAGE~.,data=PSID_reg,nvmax=40,method="forward")
forw.summary = summary(regfit.forw)
rsq_forw <- forw.summary$rsq
bic_forw <- forw.summary$bic
```

## Backward stepwise selection

```{r}
regfit.back = regsubsets(WAGE~.,data=PSID_reg,nvmax=40,method="backward")
back.summary = summary(regfit.back)
rsq_back <- back.summary$rsq
bic_back <- back.summary$bic
```

## Best Subset selection 

This would, in principle, be a possibility, but with over 130 possible variables, this leaves too many possibilities. The code would run for a long time, even for the modest number of observations

```{r, eval = FALSE}
regfit.back = regsubsets(WAGE~.,data=PSID_reg,nvmax=40,really.big = TRUE, method="exhaustive")
full.summary = summary(regfit.forw)
rsq_full <- full.summary$rsq
bic_full <- full.summary$bic
```


```{r}
res_sel <- data.frame(vars = seq(1,40), bic = bic_forw, rsq = rsq_forw, method = "Forward")
res_sel <- rbind(res_sel, data.frame(vars = seq(1,40), bic = bic_back, rsq = rsq_back, method = "Backward"))
```

Look at the resulting dataframe to understand its structure. Now we can easily plot the results.

```{r}
ggplot(res_sel, aes(x = vars, y = bic, color = method)) + 
  geom_line() +
  labs(title = "BIC for forward and backward selection")

res_sel %>% group_by(method) %>% summarise(test = which.min(bic))
```

The different methods select slightly different numbers of optimal variables. Let's check which variables were selected.

```{r}
PSIDX = model.matrix(WAGE ~ .,data = PSID_reg)    # Build the model matrix
print("Selected variables from forward procedure")
selvars.forw <- forw.summary$which[21,]
names(selvars.forw[selvars.forw==TRUE])
print("")
print("Selected variables from backward procedure")
selvars.back <- back.summary$which[19,]
names(selvars.back[selvars.back==TRUE])
```

From these lists you can see that the two lists are very similar. They only differ in two state dummies.

## Ridge Regression

The previous variable selection methods estimated a lot of different models to then compare these using information criteria. Ridge regression will estimate one model only (not by OLS any longer) but will penalise coefficients in a way to "bend" them towards 0. 

To apply this method we need to define a vector `y` to contain the dependent variable and a matrix `x` which contains all possible variables in the columns. So let's define these two variables.

```{r}
y <- PSID_reg$WAGE
x <- model.matrix(WAGE~.,data = PSID_reg)[,-1]
```

The `model.matrix` function is useful as it creates a matrix of all the explanatory variables of a model that models the dependent variable (here `WAGE`) on all other variables in the specified dataset (here `PSID_reg`). The important role this function has is to convert all categorical variables into numerical dummy variables. The `glmnet` function we are using to estimate ridge and LASSO regressions can only deal with numeric variables. The `[,-1]` at the end actually deletes the first column which is the constant.

How many rows and columns does the `x` matrix have?

rows: `r fitb(dim(x)[1])`
columns: `r fitb(dim(x)[2])`

`r hide("I need a hint")`

You can use the command  `dim(x)` to get rows and columns numbers of a matrix.

`r unhide()`

you will now use the `glmnet` function to estimate a ridge regression. While we will not go through the technical details of ridge regressions it is worthwhile to restate the optimisation problem solved by ridge regressions. We are estimating the parameters in the  following regression model

\begin{equation}
y_i = \beta_0 + \sum^p_{j=1} \beta_j x_{ij} + u_i
\end{equation}

where the regression model contains all $p$ regressors included in the matrix `x` we defined above. The parameters are estimated by minimizing

\begin{equation}
\sum^n_{i=1} \left( y_i - \beta_0 - \sum^p_{j=1} \beta_j x_{ij}\right)^2 + \lambda \sum^p_{j=1}  \beta_j^2 = RSS + \lambda \sum^p_{j=1}  \beta_j^2 
\end{equation}

This is the standard residual sum of squares (RSS) plus a penalty term which penalises against large parameter values. The parameter $\lambda$ will have to be chosen by you the empirical researcher. More on that soon.

Before you apply the following function you should briefly browse through the help (`?glmnet`) of this function to understand some of the main features. There you will learn that the input option `alpha = 0` instructs the function to implement a ridge regression. This sets the penalty term to be equal to the above $\lambda \sum^p_{j=1}  \beta_j^2$.

Let's estimate a ridge regression:

```{r}
ridge=glmnet(x,y,family="gaussian",alpha=0)
```

Does the estimated ridge regression standardise the variables?

`r hide("I need a hint")`

You can use the help function  `?glmnet` to find information on the function's default choice.

`r unhide()`

`r mcq(c("no, neither y nor the variables in x", "only y", "only the variables in x", answer = "yes, both y and the variables in x"))` 

The `glmnet` function automatically minimises the above objective function for a range of valued of $\lambda$. For each value of $\lambda$ the estimated coefficients are different. The larger the $\lambda$ the more large parameter values are penalised and therefore we will get smaller parameter estimates. 

What is the meaning of the `family = "gaussian"` option inthe call to `glmnet`?

`r hide("I need a hint")`

Consult the help function. In the Details section you will find that using this option ensures that the optimisation minimises $(1/2 n) * RSS  + \lambda * penalty$. That is the optimisation function stated above. The difference is the factor $(1/2n)$ but as that is a constant this makes no difference to the optimised parameter values.  

`r unhide()`

The following plot shows these parameter estimates for large $\lambda$s on the left and small $\lambda$s on the right.

```{r}
plot(ridge, main="Ridge and coefficients path")
```

You can see one line for each of the 136 variables included in the model. An alternative way to see the parameters is the following. First re-estimate the model for particular values of $\lambda$.

```{r}
paste("Large lambda:", ridge$lambda[25])
paste("Medium lambda:", ridge$lambda[50])
paste("Smalllambda:", ridge$lambda[75])

ridge_lar=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[25])
ridge_med=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[50])
ridge_small=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[75])
```

Now we use the `coefplot` function to plot the coefficients for each of these models.

```{r}
coefplot(ridge_small,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (small lambda)")
coefplot(ridge_med,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (mid lambda)")
coefplot(ridge_lar,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (large lambda)")
rm(ridge25,ridge50,ridge75)
```

When you compare the plots make sure you note the different scales on the horizontal axis. Larger $\lambdas$ result in smaller coefficient values. The vertical axis shows the associated variables, but there are too many to read these properly.

### Choice of lambda

The ridge regressions estimated above incorporate a trade-off. A $\lambda = 0$ would have delivered the best in-sample fit, however, this is unlikely to be the best choice when using the model for forecasting as it would over-fit the data used for estimation. Larger values of $\lambda$ will penalise against the in-sample over-fitting. But what is the best $\lambda$ to use?

You will apply a cross validation approach to establish the best value of $\lambda$. The `glmnet` package we are using here to estimate Ridge (and Lasso) regressions has a function that makes this easy to apply. Here is the code:

```{r}
set.seed(1)
cvridge=cv.glmnet(x,y,family="gaussian",alpha=0)
plot(cvridge,main="Ridge and CV criterion")
```

How many folds are used in the default setting for `cv.glmnet`?

`r mcq(c( "5", answer ="10", "20", "100"))` 

Why does `plot(cvridge,main="Ridge and CV criterion")` return the mean square error as the criterion of fit?  

`r mcq(c( "cv.glmnet will always return MSE", "MSE is the only sensible measure of fit", answer ="This is the default when minimising RSS", "This is the default when maximising the log likelihood"))` 

As you can see the `cv.glmnet` function returns estimation results for a range of possible $\lambda$ values. Let's see which is the optimal $\lambda$ by looking at `cvridge$lambda.min` which is where the $\lambda$ with the smallest MSE has been saved.

```{r}

cvridge$lambda.min
```

When calculating the natural log of that $\lambda$ you will find that to be `r log(cvridge$lambda.min)` which corresponds to the value where you can see the minimum MSE in the above plot (also signaled by the left dashed vertical line). 

As you can see from the above plot MSE stays more or less the same even for somewhat larger values of $\lambda$ and often it is seen as advantageous to impose a somewhat larger penalty. The second vertical dashed line in the above plot indicates the value of $\lambda$ where the MSE is only one standard error larger than at `cvridge$lambda.min`. This is this value:

```{r}
cvridge$lambda.1se
```

Once you have chosen a $\lambda$ you can get the coefficients and predictions from that particular model. Here we look at the first 11 coefficients from the optimal model using both $\lambda_{min}$ and $\lambda_{1se}$. To do that we use the `predict` function using the `type="coeff"` option.

```{r}
# list estimated coefficients with lambda.min
paste("lambda = ", cvridge$lambda.min)
ridge.coef.lambda.min=predict(cvridge, x, type="coeff",s=cvridge$lambda.min)[1:11,]
ridge.coef.lambda.min

# list estimated coefficients with lambda.1se
paste("lambda = ", cvridge$lambda.1se)
ridge.coef.lambda.1se=predict(cvridge, x, type="coeff",s=cvridge$lambda.1se)[1:11,]
ridge.coef.lambda.1se
```

When re-running the above code for different random seeds the optimised parameters at any $\lambda$ will remain unchanged.

`r torf(FALSE)`

`r hide("Why is that so?")`

The folds are created randomly. Everytime you divide the observations into the folds this allocation is random. For different such allocations you will find different optimised parameters. Only if you set the random seed to a particular value can you replicate results.

`r unhide()`

## LASSO



#-------------------------------------------------------------------------#
# variable selection using LASSO  (using the glmnet package)
#-------------------------------------------------------------------------#

# In glmnet we need to let alpha=1 to ensure we apply LASSO
# we standardize the regressors

lasso=glmnet(x,y,family="gaussian",alpha=1,standardize=TRUE,intercept=TRUE) 
par(mfrow=c(1,1))
plot(lasso,main="Lasso and coefficients path")

lasso25=glmnet(x,y,family="gaussian",alpha=1,lambda=lasso$lambda[25])
lasso50=glmnet(x,y,family="gaussian",alpha=1,lambda=lasso$lambda[50])
lasso75=glmnet(x,y,family="gaussian",alpha=1,lambda=lasso$lambda[75])

dev.off()
coefplot(lasso75,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="LASSO Coefficient Plot (small lambda)")
coefplot(lasso50,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="LASSO Coefficient Plot (mid lambda)")
coefplot(lasso25,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="LASSO Coefficient Plot (large lambda)")
rm(lasso25,lasso50,lasso75)

# We can choose lambda by cross-validation using cv.glmnet
# by default it uses a 10-fold cross validation and a grid of 100 lambda values
# lambda.min: the value that gives the min CV criterion
# lambda.1se, the largest value of lambda such that the error is within 
#      1 standard dev from the minimum  CV criterion

set.seed(1)
cv_lasso=cv.glmnet(x,y,family="gaussian",nfolds=10,alpha=1)
# Plot CV criterion
par(mfrow=c(1,1))
plot(cv_lasso,main="Lasso and CV criterion")

# list estimated coefficients with lambda.min
cv_lasso$lambda.min
lasso.coef.lambda.min=predict(cv_lasso, x, type="coeff",s=cv_lasso$lambda.min)[1:21,]
lasso.coef.lambda.min

# list estimated coefficients with lambda.1se
cv_lasso$lambda.1se
lasso.coef.lambda.1se=predict(cv_lasso, x, type="coeff",s=cv_lasso$lambda.1se)[1:21,]
lasso.coef.lambda.1se


#-------------------------------------------------------------------------#
# RIDGE shrinks parameters rather than selecting them (LASSO)
#-------------------------------------------------------------------------#

# In glmnet setting alpha=0 ensures we apply RIDGE

ridge=glmnet(x,y,family="gaussian",alpha=0)
plot(ridge,main="Ridge and coefficients path")

ridge25=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[25])
ridge50=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[50])
ridge75=glmnet(x,y,family="gaussian",alpha=0,lambda=ridge$lambda[75])

coefplot(ridge75,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (small lambda)")
coefplot(ridge50,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (mid lambda)")
coefplot(ridge25,intercept=FALSE,sort="magnitude",ylab="Variable",xlab="Coefficient",title="RIDGE Coefficient Plot (large lambda)")
rm(ridge25,ridge50,ridge75)

set.seed(1)
cvridge=cv.glmnet(x,y,family="gaussian",alpha=0)
plot(cvridge,main="Ridge and CV criterion")

# list estimated coefficients with lambda.min
cvridge$lambda.min
ridge.coef.lambda.min=predict(cvridge, x, type="coeff",s=cvridge$lambda.min)[1:21,]
ridge.coef.lambda.min

# list estimated coefficients with lambda.1se
cvridge$lambda.1se
ridge.coef.lambda.1se=predict(cvridge, x, type="coeff",s=cvridge$lambda.1se)[1:21,]
ridge.coef.lambda.1se


#-------------------------------------------------------------------------#
# variable selection using LASSO  (using the hdm package)
#-------------------------------------------------------------------------#

y=as.matrix(PSID[,1])
x=as.matrix(PSID[,2:21])

ylasso <- rlasso(y=y,x=x,post=TRUE)
# ylasso <- rlasso(y=y,x=x,post=TRUE,intercept=TRUE,
#              penalty = list(homoscedastic=FALSE,X.dependent.lambda = FALSE, 
#                             Lambda.start = NULL, c=1.1, gamma = 0.1/log(n)),
#              control = list(numIter=15, tol = 10^-5, threshold = NULL))

# index indicating selected variables
Iy=ylasso$index
# coefficients obtained by lasso
ylasso$coefficients

# Note: these results are very similar to lasso.coef.lambda.1se
# Need to still see how to change the automated selection of lambda here  



#---------------------------------------------------------------------------------#
# clearly in this model, we are in particular interested in estimating and 
# performing inference on the variable GRADCOLL that accounts for all confounders
#---------------------------------------------------------------------------------#

lasso_effect_p = rlassoEffects(x=x,y=y,index=c(19),method="partialling out",I3=NULL,post=TRUE)
summary(lasso_effect_p)
lasso_effect_d = rlassoEffects(x=x,y=y,index=c(19),method="double selection",I3=NULL,post=TRUE)
summary(lasso_effect_d)

```