---
title: "Double Robust LASSO"
output: webexercises::webexercises_default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(webexercises)
```


```{r, echo = FALSE, results='asis'}
# Uncomment to change widget colours:
#style_widgets(incorrect = "goldenrod", correct = "purple")
```

This exercise is part of the [ECLR](https://datasquad.github.io/ECLR/) page.

# Introduction

In the [Ridge and Lasso exercise](https://datasquad.github.io/ECLR/R_RidgeLasso/R_RidgeLasso.html) we learned how to apply Ridge and LASSO regressions to select covariates in a linear (in parameters) regression model. As you well know, regression parameters do not automatically have a causal interpretation. This is true for normal regressions where the researcher selects the covariates, regressions where covariates are selected with [subsample regression methods](https://datasquad.github.io/ECLR/R_VariableSelection_Credit/R_VariableSelection_Credit.html) as well as regressions estimated by Ridge or LASSO regressions. 

Consider a regression model of the following form 

\begin{equation}
  y_i = \delta D_i + \mathbf{x}_i\mathbf{\beta} + \epsilon_i
\end{equation}

where the variables represent

* an outcome variable of interest, $y_i$, for individual $i$
* a treatment variable, $D_i$, which represents whether individual received some treatment ($D_i=1$) or not ($D_i=1$)
* a vector of covariates associated to individual $i$, $\mathbf{x}_i$, where this includes a constant.

Here we are interested in the average treatment effect (ATE), as captured by the parameter $\delta$. It is, of course, important to understand that the coefficient $\delta$ does identify the average treatment effect of treatment $D$ only if certain assumptions are given. This exercise is not the place where these are discussed in detail. The one assumption that has consequences for this exercise is the conditional independence assumption (CIA). This implies that the potential outcomes (the outcome that eventuates if $i$ receives treatment and the outcome that eventuates if $i$ does not receive treatment) are independent of whether treatment is received as long as we condition on covariates $X$. 

This is trivially true if the treatment is randomly allocated (e.g. in a randomised controled trial). However, in the more common situation in which we are dealing with an observational study where treatments are not randomly allocated, this is unlikely to be true. In such situations the treatment variable is potentially endogenous (correlated with the error term) resulting in $\delta$ not identifying the ATE.

This is problematic if the treatment is correlated with factors that are relevant for the outcome. Here there are two situations to consider:

1. All or some of the relevant (for the outcome variable) factors that are correlated with the treatment ($D$) are not observable and hence necessarily captured in the error term. This situation is called "selection on unobservables". In such a situation, there is no way how estimating a regression like the above can recover the ATE. Alternative identifying strategies are needed (e.g. instrumental variables or difference-in-difference).
2. All relevant (for the outcome variable) factors that are correlated with the treatment ($D$) are observable and hence can be included in the set of covariates ("selection on observables"). In this situation the ATE is identified by $\delta$ if all these variables are included in $x_i$ in the above regression.

It is in the process of following the strategy in 2 that machine learning methods like the LASSO can be useful, in particular in situations where economic theory does not provide clear guidance on what these variables should be. As it is important to include all relevant variables this selection should be done as best as possible. In particular if the set of possible variables is large relative to the number of observations this is crucial.

The way this will be implemented is by making use of the following result (Frisch-Waugh-Lovell theorem). We can estimate the parameter $\delta$ using the following regression

\begin{equation}
r^y_i = \delta r^D_i + u_i
\end{equation}

where $r^y_i$ is the estimated residual of the regression $y_i= \mathbf{x}_i\mathbf{\tau}^y + res^y_i$ and $r^D_i$ is the estimated residual of the regression $D_i= \mathbf{x}_i\mathbf{\tau}^D + res^D_i$.

LASSO can be applied for these two helper regressions. This is particularly useful if you have a high-dimensional matrix of covariates $\mathbf{X}$.


# Setup

Let's load up the libraries we will be using in this project.

```{r}
# load packages
library(car)
library(AER)
library(glmnet)
library(hdm)
library(tidyverse)
library(stargazer)
library(ggplot2)
library(sandwich)
```

The package `hdm` is the package which offers the functionality we will be using. 

# Empirical example

The example we will work through here is a cross-country regression following the work by Barro and Lee (1994), [NBER Working Paper](https://www.sciencedirect.com/science/article/abs/pii/0167223194900027)) attempting to establish the determinants of  differential country growth rates. The dataset for that work is pre-loaded with the `hdm` package. We can load it using the following command.

```{r}
data(GrowthData)
str(GrowthData)
```

There are 90 observations and 63 variables (really only 62 as there is an `intercept` column).

## Exploratory analysis

This dataset contains a lot of information and the variable definitions are not immediately obvious. In [this ECLR exercise](https://datasquad.github.io/ECLR/Ex/DataManagementExample_GrowthData.html) you can find more details on a somewhat larger dataset and a link to a data dictionary ([readme.txt](https://github.com/datasquad/ECLR/blob/gh-pages/data/BARLEE/readme.txt)) from which you can derive the definitions of the variables in the `GrowthData` dataset. It is not important to understand the definitions of all variables here, but with the help of that data dictionary you can for instance establish the following definitions:

* `human65`, Average schooling years in the total population over age 25 in 1965
* `seccf65`, Percentage of "secondary school complete" in female pop.
* `freeop`, Measure of "Free trade openness". 

The two variables we are particularly interested here is the variable labelled `Outcome` which here is defined as a GDP growth rate over a decade (from 1965 to 1975) for a particular country. The covariate we are particularly interested in is `gdpsh465` which represents the initial (i.e. in 1965) level of $log(GDP per capita)$. The question here is whether countries which are poorer at the beginning of that growth period grow faster, ceteris paribus, implying a negative relation between `Outcome` ($y_i$) and `gdpsh465` ($D_i$) after controlling for relevant covariates ($\mathbf{X}$). 

The question then is what the relevant covariates are?

Before we answer this question let us explore some of the data. 

The data file has data for `r fitb(90)` countries.

After inspecting the datafile it is apparent that `GrowthData` ...

`r mcq(c("contains country names", "contains 3 letter country codes.", "contains numerical country codes", answer = "does not contain a variable which indicates which country each row represents."))`


`r hide("I need a hint")`

All variables are numeric variables. Evidently they do not contain any country names or letter abbreviations. It could be that one of the numerical variables represent a country. In the data dictionary you will actually find such a variable called `SHCODE`, but that variable is not included in `GrowthData`.

If you wanted to identify countries you could go to the data exercise linked above and attempt to match variable outcomes, say for `pop65` to identify countries. In that way you will be able to match about half of the countries.

`r unhide()`

First we shall plot a histogram of the outcome variable.

```{r}
p1 <- ggplot(GrowthData,aes(x=Outcome)) + geom_histogram()
p1
```

As you can see the majority of countries display positive growth rates. 

Let's produce a scatterplot that displays the initial level of GDP to the subsequent growth.

```{r}
p2 <- ggplot(GrowthData,aes(x=gdpsh465,y=Outcome)) + geom_point()
p2
```

From this plot it is impossible to see any obvious relationship between the initial GDP per person (`gdpsh465`) and the Growth rate over the next decade (`Outcome`). But that is perhaps not surprising as the above plot ignores any covariates that may help explain variation. Their absence may hide any such relationship.

A simple regression shows the following.

```{r}
ols1 <- lm(Outcome~gdpsh465,data = GrowthData)
stargazer(ols1,type = "text")
```

The coefficient to the initial GDP value is clearly not statistically significant. Even calculating heteroskedasticity robust standard errors (see next code chunk) does not change this.

```{r}
se_hc_ols1 <- sqrt(diag(vcovHC(ols1, type = "HC1"))) # calculate HS robust s.e.
```

## Allowing for covariates, Economic judgement

In the paper by Barro and Lee (1994), [NBER Working Paper](https://www.sciencedirect.com/science/article/abs/pii/0167223194900027)) the authors make economic arguments for the covariates that are to be included. They list the following type of variables:

* life expectancy, `lifee065`
* percentage of secondary school completions in male and female population, `seccm65` and `seccf65`
* ratio of real gross domestic investment to GDP, `invsh41`   
* ratio of government consumption (not incl defense and education) to GDP, `gvxdxe41`
* (the log of) the black market premium on foreign exchange, `log(1+bmp1l)`
* the average number of revolutions and assassinations per year, `pinstab1`

```{r}
ols2mod <- "Outcome~gdpsh465+lifee065+seccm65+seccf65+invsh41+gvxdxe41+log(1+bmp1l)+pinstab1"
ols2 <- lm(ols2mod,data = GrowthData)
```

We want to calculate heteroskedasticity robust standard errors. This is easily done using this little wrapper function for `stargazer`. Dowload this file [stargazer_HC.R](https://raw.githubusercontent.com/datasquad/ECLR/gh-pages/docs/stargazer_HC.R), save it to your working folder and then load the function using

```{r, echo = FALSE}
source("../docs/stargazer_HC.R")
```

```{r, eval = FALSE}
source("stargazer_HC.R")
```

Then you can display regression results using this function. Robust standard errors will be automatically displayed.

```{r}
stargazer_HC(ols1,ols2)
```
You now find a statistically significant effect of the the original level of GDP.

## Using LASSO for variable selection

In the previous section we used the economic expertise of Barro and Lee to decide which covariates to include in $\mathbf{X}$. You may wonder whether such expertise is actually needed. Couldn't we just include all available variables (the kitchen sink approach)? Let's try this.


```{r}
ols3mod <- "Outcome~."  # ~. indicates that you do want to include all variables in the dataset
ols3 <- lm(ols3mod,data = GrowthData)
stargazer_HC(ols1,ols2,ols3,keep = "gdpsh465")
```

While we now get a slightly negative coefficient we also get a large standard error resulting in a statistically insignificant coefficient estimate.

The aim is now to evaluate whether the economic expertise can be replaced by using LASSO, or if we can even make a case for the statistical tool of LASSO being able to identify variables that are statistically relevant and had not been identified by the economic expertise.

In order to use the `rlassoEffect` function which is part of the `hdm` package we need to define matrices with variables $y_i$, $D_i$ and $\mathbf{x_i}$. This is done here:

```{r}
y = as.matrix(GrowthData[, "Outcome"]) 
d = as.matrix(GrowthData[, "gdpsh465"])
X = as.matrix(GrowthData)[, -c(1, 2, 3)] 
varnames = colnames(GrowthData)
varnames.X = colnames(X) # produces a list of all possible covariates
```

As you can see, for the definition of `X` the first three columns of data (`Outcome`, `Intercept` and `gdpsh465`) have been excluded. An intercept is automatically added.

The process that is now implemented is that the `rlassoEffect` function will estimate two helper regressions

\begin{equation}
  \text{Direct: }y_i = \mathbf{x}_i\mathbf{\gamma} + u_i
\end{equation}

and

\begin{equation}
  \text{Indirect: }d_i = \mathbf{x}_i\mathbf{\kappa} + v_i
\end{equation}

For each of these a LASSO is applied to select variables from $\mathbf{x}_i$. The two selections will typically be different and the Post Double Selection estimator then uses the union of these two selections, $\mathbf{x}^*_i$, in a new model.

\begin{equation}
  y_i = \delta D_i + \mathbf{x}^*_i\mathbf{\beta} + \epsilon_i
\end{equation}

This is what is being achieved by the following command:

```{r}
doublesel.effect = rlassoEffect(x = X, y = y, d = d, method = "double selection")
summary(doublesel.effect)
```

As you can see, the estimated effect of the initial level of GDP (`gdpsh465`) is stronger than that identified in the model that used the expert judgement (`OLS2`). 

You may now be interested which variables this procedure selected for $\mathbf{x}^*_i$. This information is contained in `doublesel.effect$selection.index`.

```{r}
ds.sel <- varnames.X[doublesel.effect$selection.index]
ds.sel
```

Only variables `bmp1l` and `life065` were also included in the expertise driven model `OLS2`.

## Investigating the double selection method

In this section we shall reconstruct the double selection process in the above example. We wish to understand why, the particular selection (`ds.sel`) was made. To understand this we shall estimate 

In order to understand the Double Selection method

This exercise is part of the [ECLR](https://datasquad.github.io/ECLR/) page.


\begin{equation}
  \text{Direct: }y_i = \mathbf{x}_i\mathbf{\gamma} + u_i
\end{equation}

and

\begin{equation}
  \text{Indirect: }d_i = \mathbf{x}_i\mathbf{\kappa} + v_i
\end{equation}

One of the variables in `ds.sel` is `humanf65`. Which of the following is true?

`r mcq(c("humanf65 was selected as a covariate in both models", "humanf65 was selected as a covariate in model Direct", "humanf65 was selected as a covariate in model Indirect", "humanf65 was not selected in either Direct or Indirect", "humanf65 was selected in either Direct or Indirect but not both", answer = "humanf65 was selected in at least one of Direct or Indirect"))` 

`r hide("I need a hint")`

Recall that the covariates used in the post double selection model is the union of the covariate selections in the two helper models. Therefore, any variable that appears in `ds.sel` should have been selected in at least one of the two models.

`r unhide()`

Let us run the two LASSO selection models.

```{r}
GrowthData.direkt <- GrowthData %>% select(-gdpsh465) 
direkt <- rlasso(Outcome~.,data = GrowthData.direkt, post = TRUE)
```

You could look at the actual regression output with `summary(direkt)`, but here we are only interested in which variables have been selected by the LASSO. The model output `direkt$index` contains that information and we will filter out all the variables for which that index is `TRUE`.
  
```{r}
direkt$index[direkt$index==TRUE]
```

So, the only variable selected is `bmp1l`. 

Now we shall estimate the indirekt model

```{r}
GrowthData.indirekt <- GrowthData %>% select(-Outcome) 
indirekt <- rlasso(gdpsh465~.,data = GrowthData.indirekt, post = TRUE)
indirekt$index[indirekt$index==TRUE]
```

These are the remaining six variables that were selected in the Double Selection procedure.